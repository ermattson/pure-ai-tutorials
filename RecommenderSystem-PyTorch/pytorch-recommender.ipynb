{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px;\">\n",
    "  This is the accompanying code for the post titled \"Recommender Systems with PyTorch. From Data to Decisions: A Journey Through AI-Powered Recommendations\"<br>\n",
    "  You can find it <a href=\"https://pureai.substack.com/p/recommender-systems-with-pytorch\">here</a>.<br>\n",
    "  Published: December 23, 2023<br>\n",
    "  <a href=\"https://pureai.substack.com\">https://pureai.substack.com</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Jupyter notebook! If you're new to Python or don't have it installed on your system, don't worry; you can still follow along and explore the code.\n",
    "\n",
    "Here's a quick guide to getting started:\n",
    "\n",
    "- Using an Online Platform: You can run this notebook in a web browser using platforms like Google Colab or Binder. These services offer free access to Jupyter notebooks and don't require any installation.\n",
    "- Installing Python Locally: If you'd prefer to run this notebook on your own machine, you'll need to install Python. A popular distribution for scientific computing is Anaconda, which includes Python, Jupyter, and other useful tools.\n",
    "  - Download Anaconda from [here](https://www.anaconda.com/download).\n",
    "  - Follow the installation instructions for your operating system.\n",
    "  - Launch Jupyter Notebook from Anaconda Navigator or by typing jupyter notebook in your command line or terminal.\n",
    "- Opening the Notebook: Once you have Jupyter running, navigate to the location of this notebook file (.ipynb) and click on it to open.\n",
    "- Running the Code: You can run each cell in the notebook by selecting it and pressing Shift + Enter. Feel free to modify the code and experiment with it.\n",
    "- Need More Help?: If you're new to Python or Jupyter notebooks, you might find these resources helpful:\n",
    "  - [Python.org's Beginner's Guide](https://docs.python.org/3/tutorial/index.html)\n",
    "  - [Jupyter Notebook Basics](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Notebook%20Basics.html)\n",
    "\n",
    "*Note: this Notebook/Code feature the use of Poetry, a dependency and virtual environment manager. If you don't have Poetry, please install it via the Python package manager pip. Then change directories to this code, and run `poetry install --no-root`, which will install all of th required dependencies for you. You then select the poetry virtual environment as your Python kernel.\n",
    "\n",
    "Happy coding, and enjoy exploring the fascinating world of Recommender Systems with PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the device depending upon if you have a GPU or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download the MovieLens 'small' dataset from <a href=\"https://grouplens.org/datasets/movielens/latest/\">here</a>. Place the dataset folder titled 'ml-latest-small' in the same directory as the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a little bit of an exploration of our dataset to better understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.userId.nunique(), df.movieId.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 610 unique uderIds and 9724 unique movieIds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a quick histogram of the number of ratings versus their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df.rating)\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the MovieLensDataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The Movie Lens Dataset class. This class prepares the dataset for training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, users, movies, ratings):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object with user, movie, and rating data.\n",
    "        \"\"\"\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "        \"\"\"\n",
    "        users = self.users[item]\n",
    "        movies = self.movies[item]\n",
    "        ratings = self.ratings[item]\n",
    "\n",
    "        return {\n",
    "            \"users\": torch.tensor(users, dtype=torch.long),\n",
    "            \"movies\": torch.tensor(movies, dtype=torch.long),\n",
    "            \"ratings\": torch.tensor(ratings, dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's define the RecommendationSystemModel neural network in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationSystemModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_movies,\n",
    "        embedding_size=256,\n",
    "        hidden_dim=256,\n",
    "        dropout_rate=0.2,\n",
    "    ):\n",
    "        super(RecommendationSystemModel, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.embedding_size\n",
    "        )\n",
    "        self.movie_embedding = nn.Embedding(\n",
    "            num_embeddings=self.num_movies, embedding_dim=self.embedding_size\n",
    "        )\n",
    "\n",
    "        # Hidden layers\n",
    "        self.fc1 = nn.Linear(2 * self.embedding_size, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, users, movies):\n",
    "        # Embeddings\n",
    "        user_embedded = self.user_embedding(users)\n",
    "        movie_embedded = self.movie_embedding(movies)\n",
    "\n",
    "        # Concatenate user and movie embeddings\n",
    "        combined = torch.cat([user_embedded, movie_embedded], dim=1)\n",
    "\n",
    "        # Pass through hidden layers with ReLU activation and dropout\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move to define our train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_user = preprocessing.LabelEncoder()\n",
    "le_movie = preprocessing.LabelEncoder()\n",
    "df.userId = le_user.fit_transform(df.userId.values)\n",
    "df.movieId = le_movie.fit_transform(df.movieId.values)\n",
    "\n",
    "df_train, df_val = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=3, stratify=df.rating.values\n",
    ")\n",
    "\n",
    "train_dataset = MovieLensDataset(\n",
    "    users=df_train.userId.values,\n",
    "    movies=df_train.movieId.values,\n",
    "    ratings=df_train.rating.values,\n",
    ")\n",
    "\n",
    "valid_dataset = MovieLensDataset(\n",
    "    users=df_val.userId.values,\n",
    "    movies=df_val.movieId.values,\n",
    "    ratings=df_val.rating.values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to create the dataloaders that will be used during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    valid_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the recommendation model as well as define the optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_model = RecommendationSystemModel(\n",
    "    num_users=len(le_user.classes_),\n",
    "    num_movies=len(le_movie.classes_),\n",
    "    embedding_size=64,\n",
    "    hidden_dim=128,\n",
    "    dropout_rate=0.1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(recommendation_model.parameters(), lr=1e-3)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to log the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "\n",
    "# Function to log progress\n",
    "def log_progress(epoch, step, total_loss, log_progress_step, data_size, losses):\n",
    "    avg_loss = total_loss / log_progress_step\n",
    "    sys.stderr.write(\n",
    "        f\"\\r{epoch+1:02d}/{EPOCHS:02d} | Step: {step}/{data_size} | Avg Loss: {avg_loss:<6.9f}\"\n",
    "    )\n",
    "    sys.stderr.flush()\n",
    "    losses.append(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is our training loop that will train our network over 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "log_progress_step = 100\n",
    "losses = []\n",
    "train_dataset_size = len(train_dataset)\n",
    "print(f\"Training on {train_dataset_size} samples...\")\n",
    "\n",
    "recommendation_model.train()\n",
    "for e in range(EPOCHS):\n",
    "    step_count = 0  # Reset step count at the beginning of each epoch\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        output = recommendation_model(\n",
    "            train_data[\"users\"].to(device), train_data[\"movies\"].to(device)\n",
    "        )\n",
    "        # Reshape the model output to match the target's shape\n",
    "        output = output.squeeze()  # Removes the singleton dimension\n",
    "        ratings = (\n",
    "            train_data[\"ratings\"].to(torch.float32).to(device)\n",
    "        )  # Assuming ratings is already 1D\n",
    "\n",
    "        loss = loss_func(output, ratings)\n",
    "        total_loss += loss.sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increment step count by the actual size of the batch\n",
    "        step_count += len(train_data[\"users\"])\n",
    "\n",
    "        # Check if it's time to log progress\n",
    "        if (\n",
    "            step_count % log_progress_step == 0 or i == len(train_loader) - 1\n",
    "        ):  # Log at the end of each epoch\n",
    "            log_progress(\n",
    "                e, step_count, total_loss, log_progress_step, train_dataset_size, losses\n",
    "            )\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, plot the average losses. We should expect to see a decrease in the loss as training ocurrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Avg Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the Root Mean Squared Error between our predicted values versus the true values of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "recommendation_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, valid_data in enumerate(val_loader):\n",
    "        output = recommendation_model(\n",
    "            valid_data[\"users\"].to(device), valid_data[\"movies\"].to(device)\n",
    "        )\n",
    "        y_pred.append(output.sum().item() / len(valid_data[\"users\"]))\n",
    "        ratings = valid_data[\"ratings\"]\n",
    "        y_true.append(ratings.sum().item() / len(valid_data[\"users\"]))\n",
    "\n",
    "rms = mean_squared_error(y_true, y_pred, squared=False)\n",
    "print(f\"rms: {rms:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other metrics, we can look at the precision@k and recall@k. In the context of recommender systems, precision@k measures the proportion of recommended items in the top-k set that are relevant, while recall@k measures the proportion of relevant items found in the top-k recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def calculate_precision_recall(user_ratings, k, threshold):\n",
    "    user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "    n_rel = sum(true_r >= threshold for _, true_r in user_ratings)\n",
    "    n_rec_k = sum(est >= threshold for est, _ in user_ratings[:k])\n",
    "    n_rel_and_rec_k = sum(\n",
    "        (true_r >= threshold) and (est >= threshold) for est, true_r in user_ratings[:k]\n",
    "    )\n",
    "\n",
    "    precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "    recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "user_ratings_comparison = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for valid_data in val_loader:\n",
    "        users = valid_data[\"users\"].to(device)\n",
    "        movies = valid_data[\"movies\"].to(device)\n",
    "        ratings = valid_data[\"ratings\"].to(device)\n",
    "        output = recommendation_model(users, movies)\n",
    "\n",
    "        for user, pred, true in zip(users, output, ratings):\n",
    "            user_ratings_comparison[user.item()].append((pred[0].item(), true.item()))\n",
    "\n",
    "user_precisions = dict()\n",
    "user_based_recalls = dict()\n",
    "\n",
    "k = 50\n",
    "threshold = 3\n",
    "\n",
    "for user_id, user_ratings in user_ratings_comparison.items():\n",
    "    precision, recall = calculate_precision_recall(user_ratings, k, threshold)\n",
    "    user_precisions[user_id] = precision\n",
    "    user_based_recalls[user_id] = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = sum(prec for prec in user_precisions.values()) / len(\n",
    "    user_precisions\n",
    ")\n",
    "average_recall = sum(rec for rec in user_based_recalls.values()) / len(\n",
    "    user_based_recalls\n",
    ")\n",
    "\n",
    "print(f\"precision @ {k}: {average_precision:.4f}\")\n",
    "print(f\"recall @ {k}: {average_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now predict a user's top movies they haven't seen based on movies they rated highly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_top_movies(model, user_id, all_movies, seen_movies, device, k=5, batch_size=100):\n",
    "    model.eval()\n",
    "    unseen_movies = [m for m in all_movies if m not in seen_movies]\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(unseen_movies), batch_size):\n",
    "            batch_unseen_movies = unseen_movies[i:i+batch_size]\n",
    "            user_tensor = torch.tensor([user_id] * len(batch_unseen_movies)).to(device)\n",
    "            movie_tensor = torch.tensor(batch_unseen_movies).to(device)\n",
    "            predicted_ratings = model(user_tensor, movie_tensor).view(-1).tolist()\n",
    "            predictions.extend(zip(batch_unseen_movies, predicted_ratings))\n",
    "\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_k_movies = [movie_id for movie_id, _ in predictions[:k]]\n",
    "    return top_k_movies\n",
    "\n",
    "def get_movies_with_genres(movie_ids, df_movies):\n",
    "    # Select the relevant movies and create a new DataFrame\n",
    "    movies_with_genres = df_movies[df_movies['movieId'].isin(movie_ids)].copy()\n",
    "    # Concatenate movie titles with their genres\n",
    "    movies_with_genres['title_with_genres'] = movies_with_genres[['title', 'genres']].agg(' - '.join, axis=1)\n",
    "    return movies_with_genres['title_with_genres'].tolist()\n",
    "\n",
    "# Load movie titles and genres\n",
    "df_movies = pd.read_csv(\"ml-latest-small/movies.csv\")\n",
    "\n",
    "# Prepare all_movies and seen_movies\n",
    "all_movies = df['movieId'].unique().tolist()\n",
    "user_id = 1 # A random userId\n",
    "seen_movies = set(df[df['userId'] == user_id]['movieId'].tolist())\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = recommend_top_movies(\n",
    "    recommendation_model, user_id, all_movies, seen_movies, device\n",
    ")\n",
    "\n",
    "# Get movie titles with genres for recommended and seen movies\n",
    "recommended_movies_with_genres = get_movies_with_genres(recommendations, df_movies)\n",
    "\n",
    "# For the user's top 10 rated seen movies, get movies with genres\n",
    "user_top_ten_seen_movies = df[df['userId'] == user_id].sort_values(by=\"rating\", ascending=False).head(10)\n",
    "seen_movies_with_genres = get_movies_with_genres(user_top_ten_seen_movies['movieId'], df_movies)\n",
    "\n",
    "print(f\"Recommended movies:\\n\\n{recommended_movies_with_genres}\\n\\nbased on these movies the user has watched:\\n\\n{seen_movies_with_genres}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-recommender-system-TMo4QqvY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93e13a7c45d9e0650873e06d645db8577c9f6bb6b34b10a39c0a200b30a51d81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
